# ğŸ—£ï¸ Assistive Speech Recognition System

This mobile application is designed to support individuals with **cerebral palsy** by enabling seamless **real-time speech-to-text communication** with caregivers. The app facilitates both in-home and remote interaction, allowing users to express their needs more independently.

---

## ğŸš€ Features

- ğŸ¤ **Speech Recognition** â€“ Real-time voice-to-text conversion using OpenAI Whisper
- ğŸ—£ï¸ **Text-to-Speech** â€“ Caregivers can respond using synthesized voice replies
- ğŸ” **Role-Based Access** â€“ Separate login flows for Patients (CP Users) and Caregivers
- ğŸ”„ **Two-Way Communication** â€“ Supports message exchange even when the caregiver is remote
- â˜ï¸ **Cloud Sync** â€“ Stores chat logs and user profiles securely in Firebase
- ğŸ‘“ **Accessible UI** â€“ Clean, simple interface designed for accessibility and ease of use

---

## ğŸ› ï¸ Tech Stack

| Layer        | Technology         |
|--------------|--------------------|
| Frontend     | React Native       |
| Backend      | FastAPI (optional for Whisper deployment) |
| Speech-to-Text | OpenAI Whisper (local/remote model) |
| Text-to-Speech | Google Text-to-Speech (gTTS) |
| Authentication | Firebase Auth     |
| Database     | Firebase Firestore |
| Storage      | Firebase Storage (for audio files if needed) |

---

## ğŸ“± App Structure

- `Login Page` â€“ Sign up or login based on role
- `Role Selection` â€“ Choose between CP User and Caregiver
- `Home Pages` â€“ Role-specific dashboards
- `Profile & Settings` â€“ Update profile and preferences
- `Real-Time Chat` â€“ Transcribed speech is sent to caregiver with timestamp
- `Help & Support` â€“ FAQs and accessibility instructions

---

## ğŸ§‘â€ğŸ’» How to Run Locally

1. Clone the repo:

git clone https://github.com/dilsha6/vaagmaya.git
cd vaagmaya


2. Install dependencies:

npm install


3. Set up Firebase:
- Enable Firebase Authentication and Firestore
- Add your `google-services.json` or `GoogleService-Info.plist`

4. Run the app:

npx react-native run-android
or

npx react-native run-ios


---

## ğŸ’¡ Future Improvements

- AI-based error correction in transcription
- Offline Whisper model integration
- Real-time emotion tagging (tone/sentiment analysis)
- Multilingual support
- Emergency alert trigger

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ™‹â€â™€ï¸ Developed By

**Dilsha K** â€“ Final year B.Tech IT student passionate about building accessible healthcare solutions.

[GitHub](https://github.com/dilsha6) | [LinkedIn](https://www.linkedin.com/in/dilsha-k-a6100622b)
